{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iBPO6aOvZgZBy1Jtegbakb2TEjlLqcG4",
      "authorship_tag": "ABX9TyOJF8zWE6xfUtEwLPHjwsw6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sally20921/running-tests-in-computer-vision/blob/main/Google_Colab_Short_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Useful Resources\n",
        "- [Google Colab Tutorial](https://colab.research.google.com/drive/16pBJQePbqkz3QFV54L4NIkOn1kwpuRrj)"
      ],
      "metadata": {
        "id": "mKu0JSzaQvDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Virtual Machine\n",
        "\n",
        "The most powerful features of google colab is the ability to use cloud GPU for free. Like the other destkop environment you can also access most of the bash command with a `!` added in the front of the command.\n",
        "\n",
        "if you want to run a single script, you can use `!sh example.sh`.\n",
        "\n",
        "At first, turn on the GPU from `Runtime -> Change Runtime Type -> Hardware Acceleration`.\n",
        "\n",
        "The entire colab runs in a cloud VM (Virtual Machine). You will see that the current colab notebook is running on top of `Ubuntu 18.04.3 LTS` (at the time of this writing)."
      ],
      "metadata": {
        "id": "42FW_XtR2xjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l\n",
        "#list files and directories in long format\n",
        "# each file or directory, permissions, owner, group, size, modified date"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnKAC1M06Lbx",
        "outputId": "5cafd903-88d2-4d23-bd99-91ac09afcaba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Jan 11 17:02 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "# print working directory: the full pathname of the current working directory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT0EdrdA4oyU",
        "outputId": "d1de75b4-d667-43b7-e566-11ea721a500f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /\n",
        "# the forward slash(/) represents the root directory.\n",
        "# lists the contents of the root directory in a long format"
      ],
      "metadata": {
        "id": "Wd5O4-4rMzTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /etc/*release\n",
        "# the `cat` command writes file contents to the standard output.\n",
        "# `/etc/*release` matches any file in the `/etc` directory that ends with `release.`\n",
        "# `cat /etc/*release` often contain information about the distribution and its version."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChlZAULeNMFr",
        "outputId": "8af9e13a-a031-471f-cc5c-111dae69693a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DISTRIB_ID=Ubuntu\n",
            "DISTRIB_RELEASE=22.04\n",
            "DISTRIB_CODENAME=jammy\n",
            "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.3 LTS\"\n",
            "PRETTY_NAME=\"Ubuntu 22.04.3 LTS\"\n",
            "NAME=\"Ubuntu\"\n",
            "VERSION_ID=\"22.04\"\n",
            "VERSION=\"22.04.3 LTS (Jammy Jellyfish)\"\n",
            "VERSION_CODENAME=jammy\n",
            "ID=ubuntu\n",
            "ID_LIKE=debian\n",
            "HOME_URL=\"https://www.ubuntu.com/\"\n",
            "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
            "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
            "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
            "UBUNTU_CODENAME=jammy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU Details\n",
        "\n",
        "The GPU details can be accessed by `!nvidia-smi`."
      ],
      "metadata": {
        "id": "9RIMGm6VOArt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLquDDZNOMKl",
        "outputId": "b6b8052f-ef35-4b1b-9241-63b4859eef5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jan 16 03:03:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Current Resources Info\n",
        "- Python 3 Google Compute Engine backend (GPU)\n",
        "- Showing resources from 12:03PM to 12:05PM\n",
        " - System RAM: $0.9/12.7$ GB\n",
        " - GPU RAM: None\n",
        " - Disk: $26.3/78.2$ GB\n",
        "\n",
        "- NVIDIA SMI\n",
        " - Driver Version: 535.104.05\n",
        " - CUDA Version: 12.2\n",
        " - GPU Name: Tesla T4\n",
        " - Memory Usage: 0MiB/15360MiB\n"
      ],
      "metadata": {
        "id": "1p16KM3hOgcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ubuntu 22.04.3 LTS and Drivers\n",
        "\n",
        "Now that, as you are using Ubuntu, you can use any command that you use from GNU terminal. This makes life very simple.\n",
        "\n",
        "**However, for your project, you have to find a way to transfer or download your data files here in this Virtual Machine.**\n",
        "- The easiest way is to mount your google drive here in this VM and use git for version control of your codes.\n",
        "- Another easiest way is to get yoru file is to use `wget` from a file server or dropbox.\n",
        "\n",
        "To install a python package with `pip`, use the following. You can also use `apt-get` to install any package in ubuntu."
      ],
      "metadata": {
        "id": "AuOefmFlYvp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpustat\n",
        "!pip install fairseq\n",
        "!apt-get install build-essential"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt5JSvaiaVcj",
        "outputId": "ec2107c9-8125-49e6-b04f-69013da80ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpustat\n",
            "  Downloading gpustat-1.1.1.tar.gz (98 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/98.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nvidia-ml-py>=11.450.129 (from gpustat)\n",
            "  Downloading nvidia_ml_py-12.535.133-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat) (5.9.5)\n",
            "Collecting blessed>=1.17.1 (from gpustat)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat) (0.2.12)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat) (1.16.0)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.1.1-py3-none-any.whl size=26535 sha256=b43996a1202743d77c849f0947d87278080311e045fc6825be7373b5f9970536\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/d7/80/a71ba3540900e1f276bcae685efd8e590c810d2108b95f1e47\n",
            "Successfully built gpustat\n",
            "Installing collected packages: nvidia-ml-py, blessed, gpustat\n",
            "Successfully installed blessed-1.20.0 gpustat-1.1.1 nvidia-ml-py-12.535.133\n",
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.7)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291814 sha256=f9e6ed88801d7ffdee09fcc80dcd8a5c0dfc04dbcb6740d1e6300c8a861bf300\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=c223bf8b9a3553995c6b618a1db208d2429a13a9cb5630fa3a939bcab3d7a89f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gpustat\n",
        "! gpustat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iMmaqlCRCai",
        "outputId": "0c67c92d-4a6d-4fca-a8ab-330e78a96697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpustat\n",
            "  Downloading gpustat-1.1.1.tar.gz (98 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/98.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m92.2/98.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nvidia-ml-py>=11.450.129 (from gpustat)\n",
            "  Downloading nvidia_ml_py-12.535.133-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.10/dist-packages (from gpustat) (5.9.5)\n",
            "Collecting blessed>=1.17.1 (from gpustat)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat) (0.2.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.17.1->gpustat) (1.16.0)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.1.1-py3-none-any.whl size=26535 sha256=a909680b105f0af9a3ad889ed985a613744c2b31c7e98da104996bb9b8591e09\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/d7/80/a71ba3540900e1f276bcae685efd8e590c810d2108b95f1e47\n",
            "Successfully built gpustat\n",
            "Installing collected packages: nvidia-ml-py, blessed, gpustat\n",
            "Successfully installed blessed-1.20.0 gpustat-1.1.1 nvidia-ml-py-12.535.133\n",
            "\u001b[1m\u001b[37mc9639a16b986\u001b[m  Thu Jan 18 01:52:36 2024  \u001b[1m\u001b[30m535.104.05\u001b[m\n",
            "\u001b[36m[0]\u001b[m \u001b[34mTesla T4\u001b[m |\u001b[1m\u001b[31m 57°C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    0\u001b[m / \u001b[33m15360\u001b[m MB |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please check the nvidia driver version, and cuda version in current OS.\n",
        "\n",
        "Not all versions support the latest cuda, cudnn, etc.\n",
        "- Current Driver Version: 535.104.05\n",
        "- Current CUDA Version: 12.2\n",
        "- Python Version: 3.10.12\n",
        "- PyTorch Version: 2.1.0+cu121\n",
        "- PyTorch cuDNN Version: 8902\n",
        "\n",
        "Make sure you can install your preferred deep learning library with the current nvidia-driver, cuda and cudnn.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "📂 More about CUDA, cuDNN\n",
        "- CUDA: a parallel computing platform and programming model for general computing on GPU\n",
        "- cuDNN: GPU-accelerated library of primitives for deep neural network, **built on top of the CUDA framework**\n",
        "\n",
        "| CUDA                                                                | cuDNN                                                                                                                                            |\n",
        "|---------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| general-purpose platform for parallel computing on GPUs             | specifically tailored to accelerate deep learning tasks on GPUs                                                                                  |\n",
        "| speed up computing applications by harnessing the power of GPUs     | highly tuned implementations for standard routines in deep learning,  such as forward and backward attention, matmul, pooling and normalization. |\n",
        "| lower-level platform for general-purpose parallel computing on GPUs | higher-level library optimized for deep learning tasks, built on top of CUDA framework                                                           |\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "☝ More about CPU, GPU, TPU and the testing environment\n",
        "\n",
        "- CPU: often compared to the \"brains\" of a device, is responsible for executing instructions for programs from your system's memory.\n",
        "\n"
      ],
      "metadata": {
        "id": "8sBvgMCeRbbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from platform import python_version\n",
        "import torch\n",
        "\n",
        "print(\"Python Version\", python_version())\n",
        "print(\"Pytorch Version\", torch.__version__)\n",
        "print(\"Pytorch cuDNN Version\", torch.backends.cudnn.version())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bVawgaASFtK",
        "outputId": "3caa048e-88a6-4072-ecfe-1cd13dbdaf4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Version 3.10.12\n",
            "Pytorch Version 2.1.0+cu121\n",
            "Pytorch cuDNN Version 8902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJmJo5qaRfha",
        "outputId": "5442ad7b-6781-4ae6-9e60-6b1f52b55264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 18 01:53:37 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uploading Data to Virtual Machine\n",
        "\n",
        "There are largely three ways to upload existing data to Google Colab Virtual Machine.\n",
        "\n",
        "1. Upload From Machine: explore the built-in left panel of Google Colab\n",
        "2. Mount Google Drive: explore the built-in left panel of Google Colab\n",
        "3. using `wget`\n",
        "4. using `git clone` to clone git repository"
      ],
      "metadata": {
        "id": "nzJ7WKXxBElv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "eAsPFnalhctH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki4IqOE9BJNG",
        "outputId": "3fe4cc47-eaf1-477b-c371-ccf8162d5752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWrzL5P6CL2u",
        "outputId": "b61884d4-f437-49e7-b38b-9ae9d3073f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using `wget`"
      ],
      "metadata": {
        "id": "zdfxfLMHhkoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!wget https://dl.fbaipublicfiles.com/XML/codes_xnli_100\n",
        "!ls -l\n",
        "!head -n 10 codes_xnli_100\n",
        "# `head` command writes to standard output a specified number of lines or bytes of each specified files\n",
        "# to display the first 10 lines of a file\n",
        "## head /usr/local/doc/vi.txt\n",
        "# to display the first 5 lines of several files\n",
        "## head -5 *.xdh\n",
        "# or\n",
        "## head -n 5 *.xdh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4oos9cJhqAD",
        "outputId": "ce6a17e2-2504-4bc5-e7a5-36a36f29faa6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "--2024-04-03 06:02:40--  https://dl.fbaipublicfiles.com/XML/codes_xnli_100\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.163.189.108, 3.163.189.51, 3.163.189.96, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.163.189.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2024-04-03 06:02:40 ERROR 403: Forbidden.\n",
            "\n",
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Apr  1 13:24 sample_data\n",
            "head: cannot open 'codes_xnli_100' for reading: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### using `git clone` to clone git repository"
      ],
      "metadata": {
        "id": "xd7CKCrfh9Ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sally20921/running-tests-in-computer-vision"
      ],
      "metadata": {
        "id": "nYDJvswAkNq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing a library that is not in Colaboratory\n",
        "\n",
        "To import a library that is not in Colaboratory by default, you can use `!pip install` or `!apt-get install`.\n"
      ],
      "metadata": {
        "id": "qsMfNMJZnVpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pypi.python.org/pypi/pydot\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot"
      ],
      "metadata": {
        "id": "gY06J3t3n5sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Google Colab with GitHub\n",
        "\n",
        "[https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb](https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb)\n",
        "\n",
        "Google Colaboratory is designed to integrate cleanly with GitHub, allowing both loading notebooks from github and saving notebooks to github.\n",
        "\n",
        "\n",
        "## Loading Public Notebooks Directly from GitHub\n",
        "\n",
        "Colab can load public github notebooks directly, with no required authorization step.\n"
      ],
      "metadata": {
        "id": "GxnmCk7aoYnu"
      }
    }
  ]
}